---
layout: single
title:  "ELBO"
date:   2021-07-25
---

*The evidence lower bound is an important quantity at the core of a number of important algorithms used in statistical inference including expectation-maximization and variational inference. In this post, I describe its context, definition, and derivation.*

Introduction
----------

The **evidence lower bound (ELBO)** is an important quantity that lies at the core of a number of important algorithms in probabilistic inference such as [expectation-maximization](https://mbernste.github.io/posts/em/) and [variational infererence](https://mbernste.github.io/posts/variational_inference/). To understand these algorithms, it is helpful to understand the ELBO.

Before digging in, let's review the probabilistic inference task for a latent variable model. In a latent variable model, we posit that our observed data $ x $ is a realization from some random variable $X$. Moreoever, we posit the existence of another random variable $Z$ where $X$ and $Z$ are distributed according to a joint distribution $p(X, Z; \theta)$ where $\theta$ parameterizes the distribution.  Unfortunately, our data is *only* a realization of $X$, not $Z$, and therefore $Z$ remains unobserved (i.e. latent).

There are two predominant tasks that we may be interested in accomplishing:
1. Given some fixed value for $\theta$, compute the posterior distribution $p(Z \mid X ; \theta)$
2. Given that $\theta$ is unknown, find the maximum likelihood estimate of $\theta$: 

$$\text{argmax}_\theta l(\theta)$$

where $l(\theta)$ is the log-likelihood function:

$$l(\theta) := \log p(x ; \theta) = \log \int_z p(x, z; \theta) \ dz$$

Variational inference is used for Task 1 and expectation-maximization is used for Task 2. Both of these algorithms rely on the ELBO.

What is the ELBO?
-------------

To understand the evidence lower bound, we must first understand what we mean by "evidence".  The **evidence**, quite simply, is just a name given to the likelihood function evaluated at a fixed $\theta$:

$$\text{evidence} := \log p(x ; \theta)$$

Why is this quantity called the "evidence"? Intuitively, if we have chosen the right model $p$ and $\theta$, then we would expect that the marginal probability of our observed data $x$, would be high. Thus, a higher value of $\log p(x ; \theta)$ indicates, in some sense, that we may be on the right track with the model $p$ and parameters $\theta$ that we have chosen.  That is, this quantity is "evidence" that we have chosen the right model for the data.

If we happen to also know (or posit) that $Z$ follows some distribution denoted by $q$ (and that $p(x, z; \theta) := p(x \mid z ; \theta)q(z)$), then the evidence lower bound is, well, just a lower bound on the evidence that makes use of the known (or posited) $q$.  Specifically, 

$$\log p(x ; \theta) \geq E_{Z \sim q}\left[\log \frac{p(x,Z; \theta)}{q(Z)} \right]$$

where the ELBO is simply the right-hand side of the above inequality:

$${ELBO} := E_{Z \sim q}\left[\log \frac{p(x,Z; \theta)}{q(Z)} \right]$$

Derivation
-------------

We derive this lower bound as follows:

$$\begin{align*}\log p(x; \theta) &= \log \int p(x, z; \theta) \ dz \\ &= \log \int p(x, z; \theta) \frac{q(z)}{q(z)} \ dz \\ &= \log E_{Z\sim q} \left[ \frac{p(x, Z)}{q(z)}\right] \\ &\geq E_{Z\sim q} \left[\log \frac{p(x, Z)}{q(z)}\right]\end{align*}$$

This final inequality follows from [Jensen's Inequality](https://en.wikipedia.org/wiki/Jensen%27s_inequality).

The gap between the evidence and the ELBO
-------------

It turns out that the gap between the evidence and the ELBO is precisely the [Kullback-Leibler divergence](https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence) between $p(z \mid x; \theta)$ and $q(z)$.  This fact forms the basis of the [variational inference algorithm](https://mbernste.github.io/posts/variational_inference/) for approximate Bayesian inference!

<center><img src="https://raw.githubusercontent.com/mbernste/mbernste.github.io/master/images/ELBO_evidence_gap.png" alt="drawing" width="600"/></center>

This can be derived as follows:

$$\begin{align*}KL(q(z) \ \mid \mid p(z \mid x; \theta)) &:= E_{Z \sim q}\left[\log \frac{q(Z)}{p(Z \mid x; \theta)} \right] \\ &= E_{Z \sim q}\left[\log q(Z) \right] - E_{Z \sim q}\left[\log \frac{p(x, Z; \theta)}{p(x; \theta)} \right] \\ &= E_{Z \sim q}\left[\log q(Z) \right] - E_{Z \sim q}\left[\log p(x, Z; \theta) \right] +  E_{Z \sim q}\left[\log p(x; \theta) \right] \\ &= \log p(x; \theta) -  E_{Z\sim q} \left[\log \frac{p(x, Z; \theta)}{q(z)}\right] \\  &= \text{evidence} - \text{ELBO}\end{align*}$$

Jensen's inequality generalizes the statement that the secant line of a convex function lies above the graph of the function, which is Jensen's inequality for two points: the secant line consists of weighted means of the convex function (for t ∈ [0,1]),

{\displaystyle tf(x_{1})+(1-t)f(x_{2}),}tf(x_{1})+(1-t)f(x_{2}),
while the graph of the function is the convex function of the weighted means,

{\displaystyle f(tx_{1}+(1-t)x_{2}).}{\displaystyle f(tx_{1}+(1-t)x_{2}).}
Thus, Jensen's inequality is

{\displaystyle f(tx_{1}+(1-t)x_{2})\leq tf(x_{1})+(1-t)f(x_{2}).}{\displaystyle f(tx_{1}+(1-t)x_{2})\leq tf(x_{1})+(1-t)f(x_{2}).}
In the context of probability theory, it is generally stated in the following form: if X is a random variable and φ is a convex function, then

{\displaystyle \varphi (\operatorname {E} [X])\leq \operatorname {E} \left[\varphi (X)\right].}{\displaystyle \varphi (\operatorname {E} [X])\leq \operatorname {E} \left[\varphi (X)\right].}
The difference between the two sides of the inequality, {\displaystyle \operatorname {E} \left[\varphi (X)\right]-\varphi \left(\operatorname {E} [X]\right)}{\displaystyle \operatorname {E} \left[\varphi (X)\right]-\varphi \left(\operatorname {E} [X]\right)}, is called the Jensen gap.
