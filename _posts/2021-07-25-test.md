---
layout: single
title:  "My first post"
date:   2021-07-25
---

The first post for the blog.

The **evidence lower bound (ELBO)** is an important quantity that lies at the core of a number of important algorithms in probabilistic inference such as [expectation-maximization](https://mbernste.github.io/posts/em/) and [variational infererence](https://mbernste.github.io/posts/variational_inference/). To understand these algorithms, it is helpful to understand the ELBO.

Before digging in, let's review the probabilistic inference task for a latent variable model. In a latent variable model, we posit that our observed data $x$ is a realization from some random variable $X$. Moreoever, we posit the existence of another random variable $Z$ where $X$ and $Z$ are distributed according to a joint distribution $p(X, Z; \theta)$ where $\theta$ parameterizes the distribution.  Unfortunately, our data is *only* a realization of $X$, not $Z$, and therefore $Z$ remains unobserved (i.e. latent).

There are two predominant tasks that we may be interested in accomplishing:
1. Given some fixed value for $\theta$, compute the posterior distribution $p(Z \mid X ; \theta)$
2. Given that $\theta$ is unknown, find the maximum likelihood estimate of $\theta$: 

$$\text{argmax}_\theta l(\theta)$$

where $l(\theta)$ is the log-likelihood function:

$$l(\theta) := \log p(x ; \theta) = \log \int_z p(x, z; \theta) \ dz$$

Variational inference is used for Task 1 and expectation-maximization is used for Task 2. Both of these algorithms rely on the ELBO.

What is the ELBO?

